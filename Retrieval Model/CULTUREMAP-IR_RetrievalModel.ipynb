{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "def _ensure(p):\n",
    "    try:\n",
    "        __import__(p)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p, \"-q\"])\n",
    "for _p in [\"torch\", \"transformers\", \"peft\", \"tqdm\"]:\n",
    "    _ensure(_p)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Union, List, Dict, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel, DataCollatorForLanguageModeling, get_linear_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import PeftModel, PeftConfig, TaskType\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "if hasattr(torch, \"cuda\"):\n",
    "    try:\n",
    "        import torch.cuda.graphs as _cg\n",
    "        torch.cuda.is_available = lambda : False\n",
    "        torch.cuda.is_initialized = lambda : False\n",
    "        _cg.is_current_stream_capturing = lambda : False\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    qa_dir: str = \"/kaggle/input/province-dataset\"\n",
    "    base_model: str = \"cis-lmu/glot500-base\"\n",
    "    out_dir: str = \"/kaggle/working/glot500-2stage\"\n",
    "    pad_to_multiple_of: int = 8\n",
    "    mlm_max_len: int = 128\n",
    "    mlm_batch: int = 4\n",
    "    mlm_epochs: int = 2\n",
    "    mlm_lr: float = 2e-4\n",
    "    mlm_warmup_ratio: float = 0.06\n",
    "    mlm_mask_prob: float = 0.15\n",
    "    mlm_patience: int = 30\n",
    "    mlm_min_delta: float = 1e-4\n",
    "    ctr_max_len: int = 192\n",
    "    ctr_batch: int = 4\n",
    "    ctr_epochs: int = 1\n",
    "    ctr_lr: float = 2e-4\n",
    "    ctr_warmup_ratio: float = 0.06\n",
    "    temperature: float = 0.05\n",
    "    proj_dim: int = 128\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAPairDataset(Dataset):\n",
    "    def __init__(self, qa_dir: Union[str, Path], max_len: int = 192):\n",
    "        self.samples: List[Tuple[str, str]] = []\n",
    "        for fp in Path(qa_dir).glob(\"*.json\"):\n",
    "            try:\n",
    "                arr = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "            for rec in arr:\n",
    "                base_text = (rec.get(\"text\") or \"\").strip()\n",
    "                if not base_text:\n",
    "                    continue\n",
    "                for qa in rec.get(\"qa_pairs\", []) or []:\n",
    "                    q = (qa.get(\"question\") or \"\").strip()\n",
    "                    if q:\n",
    "                        self.samples.append((q, base_text))\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        q, a = self.samples[idx]\n",
    "        return {\"question\": q, \"answer\": a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, qa_dir: Union[str, Path]):\n",
    "        uniq: set = set()\n",
    "        for fp in Path(qa_dir).glob(\"*.json\"):\n",
    "            try:\n",
    "                arr = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "            for rec in arr:\n",
    "                t = (rec.get(\"text\") or \"\").strip()\n",
    "                if t:\n",
    "                    uniq.add(t)\n",
    "                for qa in rec.get(\"qa_pairs\", []) or []:\n",
    "                    q = (qa.get(\"question\") or \"\").strip()\n",
    "                    if q:\n",
    "                        uniq.add(q)\n",
    "        self.texts = list(uniq)\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx): return {\"text\": self.texts[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-10T12:17:13.537299Z",
     "iopub.status.busy": "2025-08-10T12:17:13.537015Z",
     "iopub.status.idle": "2025-08-10T12:17:13.554136Z",
     "shell.execute_reply": "2025-08-10T12:17:13.553428Z",
     "shell.execute_reply.started": "2025-08-10T12:17:13.537280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mean_pool(h, m):\n",
    "    x = m.unsqueeze(-1).type_as(h)\n",
    "    s = (h * x).sum(dim=1)\n",
    "    d = x.sum(dim=1).clamp(min=1e-6)\n",
    "    return s / d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, base_model_name: str, proj_dim: int, mlm_adapter_dir: str):\n",
    "        super().__init__()\n",
    "\n",
    "        acfg = PeftConfig.from_pretrained(mlm_adapter_dir)\n",
    "        if acfg.base_model_name_or_path != base_model_name:\n",
    "            print(f\"⚠️ Adapter was trained on {acfg.base_model_name_or_path}, \"\n",
    "                  f\"but you're loading {base_model_name}.\")\n",
    "\n",
    "        base = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "        if hasattr(base, \"gradient_checkpointing_enable\"):\n",
    "            base.gradient_checkpointing_enable()\n",
    "        if hasattr(base.config, \"use_cache\"):\n",
    "            base.config.use_cache = False\n",
    "\n",
    "        self.backbone = PeftModel.from_pretrained(base, mlm_adapter_dir)\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden // 2, proj_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden = out.hidden_states[-1]\n",
    "        pooled = mean_pool(hidden, attention_mask)\n",
    "        z = F.normalize(self.proj(pooled), dim=-1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveCollator:\n",
    "    def __init__(self, tokenizer: AutoTokenizer, max_len: int = 192, pad_to_multiple_of: int = 8):\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.pad = pad_to_multiple_of\n",
    "    def __call__(self, batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        qs = [b[\"question\"] for b in batch]\n",
    "        ans = [b[\"answer\"] for b in batch]\n",
    "        q = self.tok(qs, padding=True, truncation=True, max_length=self.max_len, return_tensors=\"pt\", pad_to_multiple_of=self.pad)\n",
    "        a = self.tok(ans, padding=True, truncation=True, max_length=self.max_len, return_tensors=\"pt\", pad_to_multiple_of=self.pad)\n",
    "        return {\"q_ids\": q[\"input_ids\"], \"q_mask\": q[\"attention_mask\"], \"a_ids\": a[\"input_ids\"], \"a_mask\": a[\"attention_mask\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora_cfg(r, a, d):\n",
    "    return LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=a,\n",
    "        lora_dropout=d,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "        task_type=TaskType.FEATURE_EXTRACTION\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlm(cfg: Cfg, tok: AutoTokenizer, lcfg: LoraConfig) -> str:\n",
    "    ds = MLMDataset(cfg.qa_dir)\n",
    "    print(f\"mlm texts: {len(ds)}\")\n",
    "    if len(ds) == 0:\n",
    "        Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)\n",
    "        tok.save_pretrained(cfg.out_dir)\n",
    "        p = Path(cfg.out_dir) / \"mlm\"\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        return str(p)\n",
    "\n",
    "    coll = DataCollatorForLanguageModeling(tokenizer=tok, mlm=True, mlm_probability=cfg.mlm_mask_prob)\n",
    "    def collate(feats):\n",
    "        texts = [f[\"text\"] for f in feats]\n",
    "        t = tok(texts, truncation=True, max_length=cfg.mlm_max_len, return_special_tokens_mask=True)\n",
    "        return coll([{\"input_ids\": i, \"special_tokens_mask\": m} for i, m in zip(t[\"input_ids\"], t[\"special_tokens_mask\"])])\n",
    "\n",
    "    dl = DataLoader(ds, batch_size=cfg.mlm_batch, shuffle=True, num_workers=0, pin_memory=False, collate_fn=collate)\n",
    "\n",
    "    dev = torch.device(\"cpu\")\n",
    "    model = AutoModelForMaskedLM.from_pretrained(cfg.base_model)\n",
    "    if hasattr(model, \"gradient_checkpointing_enable\"): model.gradient_checkpointing_enable()\n",
    "    if hasattr(model.config, \"use_cache\"): model.config.use_cache = False\n",
    "    model = get_peft_model(model, lcfg)\n",
    "    model.to(dev)\n",
    "    print(sum(\"lora_A\" in n for n, _ in model.named_parameters()), \"LoRA A params\")\n",
    "\n",
    "    steps = cfg.mlm_epochs * max(1, len(dl))\n",
    "    warm = int(cfg.mlm_warmup_ratio * steps)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.mlm_lr, weight_decay=0.01, foreach=False)\n",
    "    sch = get_linear_schedule_with_warmup(opt, num_warmup_steps=warm, num_training_steps=steps)\n",
    "\n",
    "    model.train()\n",
    "    ga = 8\n",
    "    c = 0\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    no_improve = 0\n",
    "    stopped_early = False\n",
    "\n",
    "    accum_loss = 0.0\n",
    "    micro_in_batch = 0\n",
    "\n",
    "    for e in range(cfg.mlm_epochs):\n",
    "        pbar = tqdm(dl, total=len(dl), desc=f\"mlm {e+1}/{cfg.mlm_epochs}\", leave=True)\n",
    "        for b in pbar:\n",
    "            b = {k: v.to(dev) for k, v in b.items()}\n",
    "            out = model(**b)\n",
    "            loss = out.loss / ga\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            accum_loss += float(out.loss.item())\n",
    "            micro_in_batch += 1\n",
    "\n",
    "            c += 1\n",
    "            if c % ga == 0:\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                sch.step()\n",
    "                mean_step_loss = accum_loss / max(1, micro_in_batch)\n",
    "                accum_loss = 0.0\n",
    "                micro_in_batch = 0\n",
    "\n",
    "                improved = (best_loss - mean_step_loss) > cfg.mlm_min_delta\n",
    "                if improved:\n",
    "                    best_loss = mean_step_loss\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "\n",
    "                pbar.set_postfix(loss=mean_step_loss, best=best_loss, patience=f\"{no_improve}/{cfg.mlm_patience}\")\n",
    "\n",
    "                if no_improve >= cfg.mlm_patience:\n",
    "                    print(f\"⏹️ Early stopping MLM: no improvement for {cfg.mlm_patience} steps. Best loss={best_loss:.6f}\")\n",
    "                    stopped_early = True\n",
    "                    break\n",
    "            else:\n",
    "                pbar.set_postfix(loss=float(out.loss.item()))\n",
    "\n",
    "        if stopped_early:\n",
    "            break\n",
    "\n",
    "    d = Path(cfg.out_dir) / \"mlm\"\n",
    "    model.save_pretrained(str(d))\n",
    "    tok.save_pretrained(cfg.out_dir)\n",
    "    del model, opt, sch, dl\n",
    "    return str(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T12:17:13.648221Z",
     "iopub.status.busy": "2025-08-10T12:17:13.647525Z",
     "iopub.status.idle": "2025-08-10T12:17:13.674474Z",
     "shell.execute_reply": "2025-08-10T12:17:13.673713Z",
     "shell.execute_reply.started": "2025-08-10T12:17:13.648195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_ctr(cfg: Cfg, tok: AutoTokenizer, mlm_dir: str) -> str:\n",
    "    ds = QAPairDataset(cfg.qa_dir, max_len=cfg.ctr_max_len)\n",
    "    print(f\"contrastive pairs: {len(ds)}\")\n",
    "    if len(ds) > 0:\n",
    "        print(f\"sample Q: {ds.samples[0][0]}\")\n",
    "        print(f\"sample A: {ds.samples[0][1][:120]} ...\")\n",
    "    if len(ds) == 0:\n",
    "        p = Path(cfg.out_dir) / \"final\"\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        tok.save_pretrained(str(p))\n",
    "        return str(p)\n",
    "    coll = ContrastiveCollator(tok, max_len=cfg.ctr_max_len, pad_to_multiple_of=cfg.pad_to_multiple_of)\n",
    "    dl = DataLoader(ds, batch_size=cfg.ctr_batch, shuffle=True, num_workers=0, pin_memory=False, collate_fn=coll)\n",
    "    dev = torch.device(\"cpu\")\n",
    "    enc = ContrastiveModel(cfg.base_model, cfg.proj_dim, mlm_dir)\n",
    "    enc.to(dev)\n",
    "    enc.train()\n",
    "    steps = cfg.ctr_epochs * max(1, len(dl))\n",
    "    warm = int(cfg.ctr_warmup_ratio * steps)\n",
    "    opt = torch.optim.AdamW(list(enc.backbone.parameters()) + list(enc.proj.parameters()), lr=cfg.ctr_lr, weight_decay=0.01, foreach=False)\n",
    "    sch = get_linear_schedule_with_warmup(opt, num_warmup_steps=warm, num_training_steps=steps)\n",
    "    ga = 16\n",
    "    c = 0\n",
    "    for e in range(cfg.ctr_epochs):\n",
    "        pbar = tqdm(dl, total=len(dl), desc=f\"contrastive {e+1}/{cfg.ctr_epochs}\", leave=True)\n",
    "        for b in pbar:\n",
    "            q_ids = b[\"q_ids\"].to(dev)\n",
    "            q_mask = b[\"q_mask\"].to(dev)\n",
    "            a_ids = b[\"a_ids\"].to(dev)\n",
    "            a_mask = b[\"a_mask\"].to(dev)\n",
    "            qz = enc(q_ids, q_mask)\n",
    "            az = enc(a_ids, a_mask)\n",
    "            sims = (qz @ az.t()) / cfg.temperature\n",
    "            y = torch.arange(sims.size(0), device=dev)\n",
    "            loss = (F.cross_entropy(sims, y) + F.cross_entropy(sims.t(), y)) / 2.0\n",
    "            loss = loss / ga\n",
    "            loss.backward()\n",
    "            c += 1\n",
    "            if c % ga == 0:\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                sch.step()\n",
    "            pbar.set_postfix(loss=float(loss.item() * ga))\n",
    "    p = Path(cfg.out_dir) / \"final\"\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    if isinstance(enc.backbone, PeftModel):\n",
    "        enc.backbone.save_pretrained(str(p))\n",
    "    else:\n",
    "        torch.save(enc.backbone.state_dict(), str(p / \"backbone.pt\"))\n",
    "    torch.save(enc.proj.state_dict(), str(p / \"projection_head.pt\"))\n",
    "    tok.save_pretrained(str(p))\n",
    "    del enc, opt, sch, dl\n",
    "    return str(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    cfg = Cfg()\n",
    "    Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    tok = AutoTokenizer.from_pretrained(cfg.base_model, use_fast=True)\n",
    "    if not tok.pad_token:\n",
    "        tok.pad_token = tok.eos_token if tok.eos_token else \"[PAD]\"\n",
    "    lcfg = lora_cfg(cfg.lora_r, cfg.lora_alpha, cfg.lora_dropout)\n",
    "    mlm_dir = train_mlm(cfg, tok, lcfg)\n",
    "    _ = train_ctr(cfg, tok, mlm_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T12:17:13.676344Z",
     "iopub.status.busy": "2025-08-10T12:17:13.675821Z",
     "iopub.status.idle": "2025-08-10T15:01:58.761942Z",
     "shell.execute_reply": "2025-08-10T15:01:58.761160Z",
     "shell.execute_reply.started": "2025-08-10T12:17:13.676322Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm texts: 9011\n",
      "73 LoRA A params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d81802feef40f189b2d2ae104431a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mlm 1/2:   0%|          | 0/2253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping MLM: no improvement for 30 steps. Best loss=2.285179\n",
      "contrastive pairs: 7550\n",
      "sample Q: گنبدهای نمکی در کدام نواحی استان فارس قرار دارند؟\n",
      "sample A: گنبدهای نمکی ساختارهای زمین‌شناسی گنبدی‌شکل با هستهٔ مرکزی نمک هستند که در نواحی جنوبی و شرقی استان فارس، به‌ویژه در منط ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bbcfce99e740029b0b6ecd3301ad8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "contrastive 1/1:   0%|          | 0/1888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "from peft import PeftModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_json_path = \"/kaggle/input/evaluation/evaluation_selected.json\"\n",
    "merged_json_path = \"/kaggle/input/evaluation/merged.json\"\n",
    "out_json_path = \"/kaggle/working/eval_top3_results.json\"\n",
    "base_model_id = \"cis-lmu/glot500-base\"\n",
    "lora_dir = \"/kaggle/working/glot500-2stage/final\"\n",
    "max_length = 128\n",
    "device = \"cuda\"\n",
    "batch_size = 64\n",
    "top_k = 3\n",
    "tfidf_analyzer = \"char\"\n",
    "tfidf_ngram_min = 3\n",
    "tfidf_ngram_max = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_device(name: str | None) -> str:\n",
    "    name = (name or \"auto\").lower()\n",
    "    if name == \"cpu\":\n",
    "        return \"cpu\"\n",
    "    if name == \"cuda\":\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if name == \"mps\":\n",
    "        return \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() else \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EmbedConfig:\n",
    "    base_model_id: str\n",
    "    model_dir: str\n",
    "    max_length: int\n",
    "    device: str | None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class _BaseEmbedder:\n",
    "    def __init__(self, tokenizer: AutoTokenizer, max_length: int, device: str):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glot500ZeroShot(_BaseEmbedder):\n",
    "    def __init__(self, cfg: EmbedConfig):\n",
    "        device = pick_device(cfg.device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(cfg.base_model_id, use_fast=True)\n",
    "        super().__init__(tokenizer, cfg.max_length, device)\n",
    "        self.model = AutoModel.from_pretrained(cfg.base_model_id, add_pooling_layer=False).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed_texts(self, texts: List[str]) -> torch.Tensor:\n",
    "        ids = self.tokenizer(texts, truncation=True, max_length=self.max_length,\n",
    "                             padding=True, return_tensors=\"pt\").to(self.device)\n",
    "        out = self.model(input_ids=ids[\"input_ids\"],\n",
    "                         attention_mask=ids[\"attention_mask\"],\n",
    "                         output_hidden_states=True, return_dict=True)\n",
    "        cls = out.hidden_states[-1][:, 0]\n",
    "        return F.normalize(cls, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glot500LoRA(_BaseEmbedder):\n",
    "    def __init__(self, cfg: EmbedConfig):\n",
    "        device = pick_device(cfg.device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(cfg.model_dir, use_fast=True)\n",
    "        super().__init__(tokenizer, cfg.max_length, device)\n",
    "\n",
    "        base = AutoModelForMaskedLM.from_pretrained(cfg.base_model_id)\n",
    "        self.model: PeftModel = PeftModel.from_pretrained(base, cfg.model_dir).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        proj_sd_path = Path(cfg.model_dir) / \"projection_head.pt\"\n",
    "        proj_sd = torch.load(proj_sd_path, map_location=\"cpu\")\n",
    "\n",
    "        hidden = self.model.config.hidden_size\n",
    "        mid = proj_sd[\"0.weight\"].shape[0]\n",
    "        proj_dim = proj_sd[\"2.weight\"].shape[0]\n",
    "\n",
    "        proj = nn.Sequential(\n",
    "            nn.Linear(hidden, mid),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mid, proj_dim),\n",
    "        )\n",
    "        proj.load_state_dict(proj_sd)\n",
    "        self.proj = proj.to(device).eval()\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed_texts(self, texts: List[str]) -> torch.Tensor:\n",
    "        ids = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        out = self.model(\n",
    "            input_ids=ids[\"input_ids\"],\n",
    "            attention_mask=ids[\"attention_mask\"],\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last = out.hidden_states[-1]\n",
    "        mask = ids[\"attention_mask\"].unsqueeze(-1).float()\n",
    "        pooled = (last * mask).sum(1) / mask.sum(1).clamp(min=1e-6)\n",
    "\n",
    "        z = F.normalize(self.proj(pooled), dim=-1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFBaseline:\n",
    "    def __init__(self, analyzer=\"char\", ngram_range=(3, 5), min_df=1):\n",
    "        self.vectorizer = TfidfVectorizer(analyzer=analyzer, ngram_range=ngram_range,\n",
    "                                          min_df=min_df, lowercase=False)\n",
    "        self.answer_matrix = None\n",
    "    def fit(self, questions, candidates):\n",
    "        _ = self.vectorizer.fit(questions + candidates)\n",
    "        self.answer_matrix = self.vectorizer.transform(candidates)\n",
    "    def topk(self, question, k=3):\n",
    "        qv = self.vectorizer.transform([question])\n",
    "        sims = cosine_similarity(qv, self.answer_matrix).ravel()\n",
    "        top_idx = sims.argsort()[::-1][:k]\n",
    "        return [(int(i), float(sims[i])) for i in top_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_all_with_progress(texts, embedder, batch_size=64, desc=\"Encode\"):\n",
    "    outs = []\n",
    "    for start in tqdm(range(0, len(texts), batch_size), desc=desc):\n",
    "        batch = texts[start : start + batch_size]\n",
    "        outs.append(embedder.embed_texts(batch))\n",
    "    return torch.cat(outs, dim=0)\n",
    "\n",
    "def topk_from_precomputed(q_emb, cand_embs, k=3):\n",
    "    sims = (cand_embs @ q_emb.T).squeeze(-1)\n",
    "    vals, idxs = torch.topk(sims, k=min(k, cand_embs.size(0)))\n",
    "    return [(int(i), float(v)) for i, v in zip(idxs.tolist(), vals.tolist())]\n",
    "\n",
    "def load_eval(path):\n",
    "    return json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def load_candidates_from_merged(merged_path):\n",
    "    data = json.loads(Path(merged_path).read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        texts = [obj.get(\"text\", \"\").strip() for obj in data if isinstance(obj, dict) and obj.get(\"text\", \"\").strip()]\n",
    "        if not texts:\n",
    "            raise ValueError(\"No non-empty 'text' fields found in merged.json.\")\n",
    "        print(len(texts))\n",
    "        return texts\n",
    "    elif isinstance(data, dict):\n",
    "        records = data.get(\"records\") or data.get(\"items\") or data.get(\"data\")\n",
    "        if isinstance(records, list):\n",
    "            return [str(x.get(\"text\", \"\")).strip() for x in records if isinstance(x, dict) and str(x.get(\"text\", \"\")).strip()]\n",
    "        raise ValueError(\"merged.json is a dict but doesn't contain a list under known keys.\")\n",
    "    else:\n",
    "        raise ValueError(\"merged.json must be a list or dict containing a list.\")\n",
    "\n",
    "def save_json(rows, out_path):\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_path.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T16:41:54.690126Z",
     "iopub.status.busy": "2025-08-10T16:41:54.689784Z",
     "iopub.status.idle": "2025-08-10T16:59:20.370360Z",
     "shell.execute_reply": "2025-08-10T16:59:20.369629Z",
     "shell.execute_reply.started": "2025-08-10T16:41:54.690105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encode candidates (base): 100%|██████████| 24/24 [02:44<00:00,  6.84s/it]\n",
      "Encode candidates (LoRA): 100%|██████████| 24/24 [14:09<00:00, 35.39s/it]\n",
      "Questions: 100%|██████████| 50/50 [00:19<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /kaggle/working/eval_top3_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "items = load_eval(eval_json_path)\n",
    "questions = [it[\"question\"] for it in items]\n",
    "candidate_texts = load_candidates_from_merged(merged_json_path)\n",
    "\n",
    "tfidf = TFIDFBaseline(analyzer=tfidf_analyzer, ngram_range=(tfidf_ngram_min, tfidf_ngram_max))\n",
    "tfidf.fit(questions, candidate_texts)\n",
    "\n",
    "cfg = EmbedConfig(base_model_id=base_model_id, model_dir=lora_dir, max_length=max_length, device=device)\n",
    "glot_base = Glot500ZeroShot(cfg)\n",
    "glot_lora = Glot500LoRA(cfg)\n",
    "\n",
    "cand_embs_base = embed_all_with_progress(candidate_texts, glot_base, batch_size=batch_size, desc=\"Encode candidates (base)\")\n",
    "cand_embs_lora = embed_all_with_progress(candidate_texts, glot_lora, batch_size=batch_size, desc=\"Encode candidates (LoRA)\")\n",
    "\n",
    "results_json = []\n",
    "for it in tqdm(items, desc=\"Questions\"):\n",
    "    q_id = it[\"id\"]\n",
    "    q_text = it[\"question\"]\n",
    "    gold_answer = it.get(\"answer\")\n",
    "    tfidf_topk = tfidf.topk(q_text, k=top_k)\n",
    "    q_emb_base = glot_base.embed_texts([q_text])\n",
    "    base_topk = topk_from_precomputed(q_emb_base, cand_embs_base, k=top_k)\n",
    "    q_emb_lora = glot_lora.embed_texts([q_text])\n",
    "    lora_topk = topk_from_precomputed(q_emb_lora, cand_embs_lora, k=top_k)\n",
    "    def pack(topk_list):\n",
    "        return [{\"candidate_id\": int(idx), \"text\": candidate_texts[idx], \"similarity\": float(score)} for idx, score in topk_list]\n",
    "    results_json.append({\n",
    "        \"id\": q_id, \"question\": q_text, \"gold_answer\": gold_answer,\n",
    "        \"results\": {\n",
    "            \"tfidf_top3\": pack(tfidf_topk),\n",
    "            \"glot500_base_top3\": pack(base_topk),\n",
    "            \"glot500_lora_top3\": pack(lora_topk)\n",
    "        }\n",
    "    })\n",
    "\n",
    "save_json(results_json, out_json_path)\n",
    "print(f\"saved: {Path(out_json_path).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Duplicate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dir = \"/kaggle/input/province-dataset\"\n",
    "model_dir = \"/kaggle/working/glot500-2stage/final\"\n",
    "base_model = \"cis-lmu/glot500-base\"\n",
    "proj_dim = 128\n",
    "out_csv = \"duplicates_finetuned.csv\"\n",
    "threshold = 0.90\n",
    "batch_size = 32\n",
    "max_len = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool(h, m):\n",
    "    x = m.unsqueeze(-1).type_as(h)\n",
    "    s = (h * x).sum(dim=1)\n",
    "    d = x.sum(dim=1).clamp(min=1e-6)\n",
    "    return s / d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, base_model_name, proj_dim, adapter_dir):\n",
    "        super().__init__()\n",
    "        acfg = PeftConfig.from_pretrained(adapter_dir)\n",
    "        if acfg.base_model_name_or_path != base_model_name:\n",
    "            print(f\"⚠️ Adapter trained on {acfg.base_model_name_or_path}, \"\n",
    "                  f\"but loading base {base_model_name}.\")\n",
    "        base = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "        if hasattr(base, \"gradient_checkpointing_enable\"):\n",
    "            base.gradient_checkpointing_enable()\n",
    "        if hasattr(base.config, \"use_cache\"):\n",
    "            base.config.use_cache = False\n",
    "        self.backbone = PeftModel.from_pretrained(base, adapter_dir)\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden // 2, proj_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden = out.hidden_states[-1]\n",
    "        pooled = mean_pool(hidden, attention_mask)\n",
    "        z = F.normalize(self.proj(pooled), dim=-1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts(qa_dir_path):\n",
    "    texts = []\n",
    "    idx = 0\n",
    "    for fp in sorted(Path(qa_dir_path).glob(\"*.json\")):\n",
    "        try:\n",
    "            arr = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "        for rec in arr:\n",
    "            t = (rec.get(\"text\") or \"\").strip()\n",
    "            if t:\n",
    "                texts.append((idx, t))\n",
    "                idx += 1\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_texts(texts, tokenizer, model, max_len=192, pad_to_multiple_of=8, batch_size=32, device=\"cpu\"):\n",
    "    embs = []\n",
    "    model.eval().to(device)\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tok = tokenizer(batch, padding=True, truncation=True, max_length=max_len,\n",
    "                        return_tensors=\"pt\", pad_to_multiple_of=pad_to_multiple_of)\n",
    "        tok = {k: v.to(device) for k, v in tok.items()}\n",
    "        z = model(tok[\"input_ids\"], tok[\"attention_mask\"])\n",
    "        embs.append(z.cpu())\n",
    "    return torch.cat(embs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_texts(qa_dir)\n",
    "if not data:\n",
    "    print(\"No texts found.\")\n",
    "else:\n",
    "    ids, texts = zip(*data)\n",
    "    print(f\"Loaded {len(texts)} texts\")\n",
    "\n",
    "    tok_path = model_dir if (Path(model_dir) / \"tokenizer.json\").exists() else base_model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_path, use_fast=True)\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else \"[PAD]\"\n",
    "\n",
    "    model = ContrastiveModel(base_model, proj_dim, str(model_dir))\n",
    "    proj_file = Path(model_dir) / \"projection_head.pt\"\n",
    "    if proj_file.exists():\n",
    "        model.proj.load_state_dict(torch.load(proj_file, map_location=\"cpu\"))\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{proj_file} not found.\")\n",
    "\n",
    "    device = \"cpu\"\n",
    "    Z = encode_texts(list(texts), tokenizer, model, max_len=max_len, batch_size=batch_size, device=device)\n",
    "    Z = F.normalize(Z, dim=-1)\n",
    "\n",
    "    radius = 1.0 - threshold\n",
    "    nn = NearestNeighbors(metric=\"cosine\", radius=radius, n_jobs=-1)\n",
    "    nn.fit(Z.numpy())\n",
    "\n",
    "    pairs = []\n",
    "    for i in tqdm(range(Z.shape[0]), desc=\"Searching\"):\n",
    "        distances, indices = nn.radius_neighbors(Z[i].numpy().reshape(1, -1), radius=radius, return_distance=True)\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        for j, dist in zip(indices[0], distances[0]):\n",
    "            if j <= i:\n",
    "                continue\n",
    "            sim = 1.0 - float(dist)\n",
    "            if sim >= threshold:\n",
    "                pairs.append((ids[i], ids[j], texts[i], texts[j], sim))\n",
    "\n",
    "    with open(out_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"id1\", \"id2\", \"text1\", \"text2\", \"similarity\"])\n",
    "        for row in pairs:\n",
    "            w.writerow([row[0], row[1], row[2], row[3], f\"{row[4]:.6f}\"])\n",
    "\n",
    "    print(f\"Wrote {len(pairs)} pairs to {out_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8042280,
     "sourceId": 12723968,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8044519,
     "sourceId": 12727275,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
