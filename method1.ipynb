{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IyLAHeew6z9J"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OIuZavW5k7Co"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Optional, List, Dict, Any, Set\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from rich.console import Console\n",
        "from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn\n",
        "from rich.table import Table\n",
        "from PyPDF2 import PdfReader\n",
        "from unsloth import FastModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn, TimeRemainingColumn\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "KU5iFS1bk9Yu"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    \"pdf_path\": Path(os.getenv(\"PDF_PATH\", \"./books/kohgiloye.pdf\")),\n",
        "    \"province\": os.getenv(\"PROVINCE\", \"کهگیلویه‌و‌بویراحمد\"),\n",
        "    \"start_page\": int(os.getenv(\"START_PAGE\", 10)),\n",
        "    \"end_page\": 103 if os.getenv(\"END_PAGE\") in (None, \"\") else int(os.getenv(\"END_PAGE\")),\n",
        "    \"chunk_size\": int(os.getenv(\"CHUNK_SIZE\", 2000)),\n",
        "    \"overlap_size\": int(os.getenv(\"OVERLAP_SIZE\", 50)),\n",
        "    \"max_seq_length\": int(os.getenv(\"MAX_SEQ_LENGTH\", 2048)),\n",
        "    \"max_new_tokens\": int(os.getenv(\"MAX_NEW_TOKENS\", 2048)),\n",
        "    \"model_name\": os.getenv(\"MODEL_NAME\", \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\"),\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"log_file\": os.getenv(\"LOG_FILE\", \"extraction.log\"),\n",
        "    \"workers\": int(os.getenv(\"WORKERS\", 1)),\n",
        "    \"partial_save_interval\": int(os.getenv(\"PARTIAL_SAVE_INTERVAL\", 10))\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "MDtOzUKHlRyG"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "    handlers=[\n",
        "        logging.FileHandler(CONFIG[\"log_file\"]),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "console = Console()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_Y0rJ8B4lWhc"
      },
      "outputs": [],
      "source": [
        "def clean_json_string(s: str) -> str:\n",
        "    \"\"\"Remove JavaScript‐style comments and stray commas so the JSON can be parsed reliably.\"\"\"\n",
        "    s = re.sub(r\"//.*\", \"\", s)\n",
        "    s = re.sub(r\"/\\*.*?\\*/\", \"\", s, flags=re.DOTALL)\n",
        "    s = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def extract_json_block(text: str) -> str | None:\n",
        "    \"\"\"Scan through a string of mixed content and return the longest balanced `{…}` JSON snippet, or None if none found.\"\"\"\n",
        "    blocks = []\n",
        "    start = None\n",
        "    depth = 0\n",
        "    for i, ch in enumerate(text):\n",
        "        if ch == '{':\n",
        "            if depth == 0:\n",
        "                start = i\n",
        "            depth += 1\n",
        "        elif ch == '}' and depth > 0:\n",
        "            depth -= 1\n",
        "            if depth == 0 and start is not None:\n",
        "                blocks.append(text[start : i + 1])\n",
        "                start = None\n",
        "    if not blocks:\n",
        "        return None\n",
        "    return max(blocks, key=len)\n",
        "\n",
        "\n",
        "def clean_json_block(raw: str) -> str:\n",
        "    \"\"\"Strip out any Markdown-style code fences (```…```) from around a raw JSON block.\"\"\"\n",
        "    s = raw.strip()\n",
        "    if s.startswith(\"```\"):\n",
        "        lines = s.splitlines()\n",
        "        if lines[0].startswith(\"```\"):\n",
        "            lines = lines[1:]\n",
        "        if lines and lines[-1].startswith(\"```\"):\n",
        "            lines = lines[:-1]\n",
        "        s = \"\\n\".join(lines)\n",
        "    return s\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(path: Path, start: int, end: Optional[int]) -> str:\n",
        "    \"\"\"Read text from pages `start` through `end` of a PDF file and concatenate them into one string.\"\"\"\n",
        "    reader = PdfReader(str(path))\n",
        "    pages = reader.pages[start-1 : end] if end else reader.pages[start-1 :]\n",
        "    texts = []\n",
        "    for page in pages:\n",
        "        try:\n",
        "            texts.append(page.extract_text() or \"\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to extract page text: {e}\")\n",
        "    return \"\\n\".join(texts)\n",
        "\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Collapse all whitespace into single spaces and strip out any digits.\"\"\"\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def chunk_text(text: str, max_chars: int, overlap: int) -> List[str]:\n",
        "    \"\"\"Break `text` into word‐aligned chunks up to `max_chars` long, preserving an `overlap` of characters for context.\"\"\"\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return []\n",
        "\n",
        "    chunks: List[str] = []\n",
        "    current: List[str] = []\n",
        "    length = 0\n",
        "\n",
        "    for w in words:\n",
        "        if length + len(w) + 1 > max_chars:\n",
        "            chunk = \" \".join(current)\n",
        "            chunks.append(chunk)\n",
        "            if overlap > 0 and len(chunk) > overlap:\n",
        "                carry = chunk[-overlap:]\n",
        "                current = carry.split()\n",
        "                length = sum(len(x) + 1 for x in current)\n",
        "            else:\n",
        "                current = []\n",
        "                length = 0\n",
        "        current.append(w)\n",
        "        length += len(w) + 1\n",
        "\n",
        "    if current:\n",
        "        chunks.append(\" \".join(current))\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "3_x9MjBTla6N"
      },
      "outputs": [],
      "source": [
        "JSON_SCHEMA_EXACT = \"\"\"\n",
        "نمونه ساختار دقیق JSON برای هر استان:\n",
        "\n",
        "{{\n",
        "  \"title\": \"string\",\n",
        "  \"location\": {{ \"province\":\"string\",\"city\":\"string\" }},\n",
        "  \"geographical_features\":[\n",
        "    {{ \"name\":\"string\",\"items\":[{{ \"name\":\"string\",\"images\":[\"string\"] }}]}}\n",
        "  ],\n",
        "  \"natural_resources\":[\n",
        "    {{ \"name\":\"string\",\"description\":[\"string\"] }}\n",
        "  ],\n",
        "  \"vegetation\":[\"string\"],\n",
        "  \"topography\":[\n",
        "    {{ \"name\":\"string\",\"description\":[\"string\"] }}\n",
        "  ],\n",
        "  \"tourist_attractions\":[\n",
        "    {{ \"name\":\"string\",\"images\":[\"string\"],\"year_built\":\"string\",\n",
        "       \"constructor\":\"string\",\"architect\":\"string\",\"description\":\"string\" }}\n",
        "  ],\n",
        "  \"climate_impacts\":[\n",
        "    {{ \"impact\":\"string\",\"description\":[\"string\"] }}\n",
        "  ],\n",
        "  \"additional_info\":{{\n",
        "    \"books_source\":\"string\",\"other_sources\":[\"string\"]\n",
        "  }}\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "EXAMPLE_JSON = \"\"\"\n",
        "مثال پرشده برای «ویژگی‌های جغرافیایی کهگیلویه‌و‌بویراحمد:\n",
        "\n",
        "{{\n",
        "  \"title\": \"ویژگی‌های جغرافیایی کهگیلویه‌و‌بویراحمد\",\n",
        "  \"location\": {{ \"province\":\"کهگیلویه‌و‌بویراحمد\",\"city\":\"یاسوج\" }},\n",
        "  \"geographical_features\":[\n",
        "    {{\n",
        "      \"name\": \"رودخانه‌ها\",\n",
        "      \"items\":[{{ \"name\":\"رود مارون\",\"images\": [] }}]\n",
        "    }},\n",
        "    {{\n",
        "      \"name\": \"کوه‌ها\",\n",
        "      \"items\":[{{ \"name\":\"کوه سرخ\",\"images\": [] }}]\n",
        "    }}\n",
        "  ],\n",
        "  \"natural_resources\": [],\n",
        "  \"vegetation\": [],\n",
        "  \"topography\": [],\n",
        "  \"tourist_attractions\": [],\n",
        "  \"climate_impacts\": [],\n",
        "  \"additional_info\": {{ \"books_source\":\"\",\"other_sources\":[] }}\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "شما مدل Gemma-3 هستید و **تنها** باید یک شیء JSON یکتا و **معتبر** تولید کنید.\n",
        "۱. کلیدها و مقدارهای رشته‌ای حتماً با گیومهٔ دوگانه (\"\") باشند.\n",
        "۲. اگر داده‌ای وجود ندارد، از \"\" یا [] استفاده کنید؛ **هرگز** {{}} خالی ننویسید.\n",
        "۳. اگر برای فیلدی اطمینان کمتر از ۹۰٪ دارید یا داده نیست، آن را \"\" یا [] بگذارید.\n",
        "۴. حتماً حداقل یک مورد واقعی برای هر لیست استخراج‌شده در متن بیاورید.\n",
        "۵. **هرگز** JSON را داخل code fence (```…```) یا تگ Markdown قرار ندهید—فقط جسم خالص JSON را برگردانید!\n",
        "۶. فقط JSON خالص، بدون توضیح یا کامنت.\n",
        "۷. **هرگز** تنها مجموعهٔ نمونه (EXAMPLE_JSON) را به‌عنوان خروجی نهایی برنگردانید؛ حتماً داده‌های استخراج‌شده از متن ورودی را نمایش دهید.\n",
        "\n",
        "{schema}\n",
        "\n",
        "{example}\n",
        "\n",
        "حال با متن زیر، دقیقاً یک شیء JSON منطبق بر ساختار فوق برگردانید:\n",
        "\"\"\"\n",
        "\n",
        "def build_prompt(province: str, chunk: str) -> str:\n",
        "    \"\"\"\n",
        "    Construct the model’s input prompt for a given province and text chunk.\n",
        "\n",
        "    This will insert the exact JSON schema and example into the template,\n",
        "    then append the source text under a clear ‘متن منبع’ header so the model\n",
        "    knows exactly what to parse and where the data applies.\n",
        "\n",
        "    :param province: The name of the province to contextualize the prompt.\n",
        "    :param chunk: The segment of source text to include.\n",
        "    :return: A ready-to-send prompt string combining template, example, and text.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        PROMPT_TEMPLATE.format(\n",
        "            province=province,\n",
        "            schema=JSON_SCHEMA_EXACT,\n",
        "            example=EXAMPLE_JSON,\n",
        "        )\n",
        "        + \"\\n--- متن منبع ---\\n\"\n",
        "        + chunk\n",
        "        + \"\\n\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "console.log(f\"Loading model {CONFIG['model_name']} on {CONFIG['device']}...\")\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name=CONFIG['model_name'],\n",
        "    max_seq_length=CONFIG['max_seq_length'],\n",
        "    load_in_4bit=True,\n",
        "    load_in_8bit=False,\n",
        "    full_finetuning=False,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"gemma-3\")\n",
        "console.log(\"Model loaded Succesfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "x1eaYl2-lyY5"
      },
      "outputs": [],
      "source": [
        "seen_entries = {\n",
        "    \"geographical_features\": set(),\n",
        "    \"natural_resources\": set(),\n",
        "    \"vegetation\": set(),\n",
        "    \"topography\": set(),\n",
        "    \"tourist_attractions\": set(),\n",
        "    \"climate_impacts\": set(),\n",
        "}\n",
        "seen_subitems = set()\n",
        "\n",
        "def process_chunk(chunk: str, idx: int) -> Optional[Dict[str, Any]]:\n",
        "    prompt = build_prompt(CONFIG['province'], chunk)\n",
        "    if idx <= 10:\n",
        "        console.rule(f\"[bold green]Chunk {idx} Prompt[/]\")\n",
        "        console.print(prompt, overflow=\"fold\")\n",
        "        console.rule()\n",
        "\n",
        "    inp = tokenizer.apply_chat_template(\n",
        "        [{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":prompt}]}],\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    inputs = tokenizer([inp], return_tensors=\"pt\").to(CONFIG['device'])\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=1.0,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "        )\n",
        "    decoded = tokenizer.batch_decode(out)[0]\n",
        "\n",
        "    marker = \"<start_of_turn>model\"\n",
        "    content = decoded.split(marker,1)[1] if marker in decoded else decoded\n",
        "    content = content.replace(\"<end_of_turn>\", \"\").strip()\n",
        "\n",
        "    if idx <= 10:\n",
        "        console.rule(f\"[bold blue]Chunk {idx} Raw Output[/]\")\n",
        "        console.print(content, overflow=\"fold\")\n",
        "        console.rule()\n",
        "\n",
        "    raw_json = extract_json_block(content)\n",
        "    if not raw_json:\n",
        "        console.log(f\"[red]⚠️ Chunk {idx}: no JSON block. Snippet: {content[:200]}...\")\n",
        "        cleanup_gpu(inputs, out)\n",
        "        return None\n",
        "\n",
        "    raw_json = clean_json_block(raw_json)\n",
        "    raw_json = clean_json_string(raw_json)\n",
        "\n",
        "    if idx <= 10:\n",
        "        console.rule(f\"[bold yellow]Chunk {idx} Extracted JSON Block[/]\")\n",
        "        console.print(raw_json, overflow=\"fold\")\n",
        "        console.rule()\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(raw_json)\n",
        "    except json.JSONDecodeError as e:\n",
        "        console.log(f\"[red]Chunk {idx} JSON parse error:[/] {e}\")\n",
        "        parsed = None\n",
        "\n",
        "    if parsed:\n",
        "        for section in [\"geographical_features\",\"natural_resources\",\"topography\",\n",
        "                        \"tourist_attractions\",\"climate_impacts\"]:\n",
        "            new_list = []\n",
        "            for item in parsed.get(section, []):\n",
        "                name = item.get(\"name\")\n",
        "                if not name or name in seen_entries[section]:\n",
        "                    continue\n",
        "                seen_entries[section].add(name)\n",
        "\n",
        "                if section == \"geographical_features\":\n",
        "                    kept_subs = []\n",
        "                    for sub in item.get(\"items\", []):\n",
        "                        subname = sub.get(\"name\")\n",
        "                        if subname and subname not in seen_subitems:\n",
        "                            seen_subitems.add(subname)\n",
        "                            kept_subs.append(sub)\n",
        "                    item[\"items\"] = kept_subs\n",
        "                    if kept_subs:\n",
        "                        new_list.append(item)\n",
        "                else:\n",
        "                    new_list.append(item)\n",
        "            parsed[section] = new_list\n",
        "\n",
        "        for section in [\"vegetation\"]:\n",
        "            new_list = []\n",
        "            for val in parsed.get(section, []):\n",
        "                if not val:\n",
        "                    continue\n",
        "                val_key = json.dumps(val, sort_keys=True)\n",
        "                if val_key not in seen_entries[section]:\n",
        "                    seen_entries[section].add(val_key)\n",
        "                    new_list.append(val)\n",
        "            parsed[section] = new_list\n",
        "\n",
        "    if idx <= 10 and parsed is not None:\n",
        "        console.rule(f\"[bold magenta]Chunk {idx} Parsed & Deduped JSON[/]\")\n",
        "        console.print(json.dumps(parsed, ensure_ascii=False, indent=2))\n",
        "        console.rule()\n",
        "\n",
        "    cleanup_gpu(inputs, out)\n",
        "    return parsed\n",
        "\n",
        "def cleanup_gpu(inputs, out):\n",
        "    \"\"\"Helper to free GPU memory immediately.\"\"\"\n",
        "    import gc\n",
        "    del inputs, out\n",
        "    gc.collect()\n",
        "    if CONFIG['device'].startswith(\"cuda\"):\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "NEYL5PXm0wvy"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Orchestrate the end-to-end extraction for a given province:\n",
        "      1. Load and normalize text from the configured PDF pages.\n",
        "      2. Split that text into overlapping chunks.\n",
        "      3. Send each chunk to the model, filtering out any duplicate entries.\n",
        "      4. Aggregate all unique results into one JSON file.\n",
        "      5. Display progress during processing and a summary table at the end.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    console.rule(\"[bold green]Starting Province Extraction[/]\")\n",
        "\n",
        "    text = extract_text_from_pdf(CONFIG['pdf_path'], CONFIG['start_page'], CONFIG['end_page'])\n",
        "    text = normalize_text(text)\n",
        "    chunks = chunk_text(text, CONFIG['chunk_size'], CONFIG['overlap_size'])\n",
        "\n",
        "    console.print(f\"[bold]Province:[/] {CONFIG['province']}\")\n",
        "    console.print(f\"[bold]Total chunks to process:[/] {len(chunks)}\\n\")\n",
        "\n",
        "    combined: Dict[str, Any] = {\"province\": CONFIG['province']}\n",
        "    partials: List[Dict[str, Any]] = []\n",
        "\n",
        "    with Progress(\n",
        "        SpinnerColumn(style=\"bold green\"),\n",
        "        TextColumn(\"[progress.description]{task.description}\"),\n",
        "        BarColumn(bar_width=None),\n",
        "        TextColumn(\"[bold magenta]{task.completed}/{task.total} chunks\"),\n",
        "        TimeElapsedColumn(),\n",
        "        TimeRemainingColumn(),\n",
        "        console=console\n",
        "    ) as progress:\n",
        "        task_id = progress.add_task(\"Extracting chunks\", total=len(chunks))\n",
        "        for idx, chunk in enumerate(chunks, start=1):\n",
        "            result = process_chunk(chunk, idx)\n",
        "            if result:\n",
        "                partials.append(result)\n",
        "            progress.advance(task_id)\n",
        "\n",
        "    for part in partials:\n",
        "        for key, value in part.items():\n",
        "            if key == \"province\":\n",
        "                continue\n",
        "            if key not in combined:\n",
        "                combined[key] = value\n",
        "            elif isinstance(value, list) and isinstance(combined[key], list):\n",
        "                combined[key].extend(value)\n",
        "\n",
        "    out_path = Path(f\"./{CONFIG['province']}_dataset.json\")\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(combined, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    total_items = sum(\n",
        "        len(v) if isinstance(v, list) else 1\n",
        "        for k, v in combined.items()\n",
        "        if k != \"province\"\n",
        "    )\n",
        "\n",
        "    console.rule(\"[bold green]Extraction Complete[/]\")\n",
        "    console.print(f\"• Saved to: [bold]{out_path}[/]\")\n",
        "    console.print(f\"• Items extracted: [bold]{total_items}[/]\")\n",
        "    console.print(f\"• Chunks processed: [bold]{len(chunks)}[/]\")\n",
        "    console.print(f\"• Elapsed time: [bold]{elapsed:.2f}s[/]\\n\")\n",
        "\n",
        "    table = Table(title=\"✅ Extraction Summary\")\n",
        "    table.add_column(\"Province\", style=\"cyan\")\n",
        "    table.add_column(\"Items\", justify=\"right\", style=\"magenta\")\n",
        "    table.add_column(\"Chunks\", justify=\"right\", style=\"magenta\")\n",
        "    table.add_column(\"Time (s)\", justify=\"right\", style=\"magenta\")\n",
        "    table.add_row(\n",
        "        CONFIG[\"province\"],\n",
        "        str(total_items),\n",
        "        str(len(chunks)),\n",
        "        f\"{elapsed:.2f}\"\n",
        "    )\n",
        "    console.print(table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "302cf2d3357249eba3de0db3b40e51c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "381b63b4d6b74c0ebbec72cc81548b4a": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_302cf2d3357249eba3de0db3b40e51c2",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">⠏</span> Extracting chunks <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">31/44 chunks</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:20:15</span> <span style=\"color: #008080; text-decoration-color: #008080\">-:--:--</span>\n</pre>\n",
                  "text/plain": "\u001b[1;32m⠏\u001b[0m Extracting chunks \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1;35m31/44 chunks\u001b[0m \u001b[33m0:20:15\u001b[0m \u001b[36m-:--:--\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
