{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMe9P3XZG-Y7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Optional, List, Dict, Any\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import openai\n",
        "from PyPDF2 import PdfReader\n",
        "from rich.console import Console\n",
        "from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn, TimeRemainingColumn\n",
        "from rich.table import Table\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD-YhEC1HHAC"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    \"pdf_path\": Path(os.getenv(\"PDF_PATH\", \"./books/Isfahan.pdf\")),\n",
        "    \"province\": os.getenv(\"PROVINCE\", \"اصفهان\"),\n",
        "    \"start_page\": int(os.getenv(\"START_PAGE\", 11)),\n",
        "    \"end_page\": None if os.getenv(\"END_PAGE\") in (None, \"\") else int(os.getenv(\"END_PAGE\")),\n",
        "    \"chunk_size\": int(os.getenv(\"CHUNK_SIZE\", 2000)),\n",
        "    \"overlap_size\": int(os.getenv(\"OVERLAP_SIZE\", 50)),\n",
        "    \"model\": os.getenv(\"GPT_MODEL\", \"gpt-4\"),\n",
        "    \"max_tokens\": int(os.getenv(\"MAX_TOKENS\", 1024)),\n",
        "    \"temperature\": float(os.getenv(\"TEMPERATURE\", 1.0)),\n",
        "    \"workers\": int(os.getenv(\"WORKERS\", 1)),\n",
        "    \"log_file\": os.getenv(\"LOG_FILE\", \"extraction.log\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-i02m9oHK8W"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"your_api_key_here\")\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "    handlers=[\n",
        "        logging.FileHandler(CONFIG[\"log_file\"]),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "console = Console()\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    raise RuntimeError(\"OPENAI_API_KEY is not set. Please assign your API key to the OPENAI_API_KEY variable.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydB1pquBHRGW"
      },
      "outputs": [],
      "source": [
        "def clean_json_string(s: str) -> str:\n",
        "    s = re.sub(r\"//.*\", \"\", s)\n",
        "    s = re.sub(r\"/\\*.*?\\*/\", \"\", s, flags=re.DOTALL)\n",
        "    s = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def extract_json_block(text: str) -> Optional[str]:\n",
        "    blocks = []\n",
        "    start = None\n",
        "    depth = 0\n",
        "    for i, ch in enumerate(text):\n",
        "        if ch == '{':\n",
        "            if depth == 0:\n",
        "                start = i\n",
        "            depth += 1\n",
        "        elif ch == '}' and depth > 0:\n",
        "            depth -= 1\n",
        "            if depth == 0 and start is not None:\n",
        "                blocks.append(text[start:i+1])\n",
        "                start = None\n",
        "    if not blocks:\n",
        "        return None\n",
        "    return max(blocks, key=len)\n",
        "\n",
        "\n",
        "def clean_json_block(raw: str) -> str:\n",
        "    s = raw.strip()\n",
        "    if s.startswith(\"```\"):\n",
        "        lines = s.splitlines()\n",
        "        if lines[0].startswith(\"```\"):\n",
        "            lines = lines[1:]\n",
        "        if lines and lines[-1].startswith(\"```\"):\n",
        "            lines = lines[:-1]\n",
        "        s = \"\\n\".join(lines)\n",
        "    return s\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(path: Path, start: int, end: Optional[int]) -> str:\n",
        "    reader = PdfReader(str(path))\n",
        "    pages = reader.pages[start-1:end] if end else reader.pages[start-1:]\n",
        "    texts = []\n",
        "    for page in pages:\n",
        "        try:\n",
        "            texts.append(page.extract_text() or \"\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to extract page text: {e}\")\n",
        "    return \"\\n\".join(texts)\n",
        "\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InQTe4GiHVX5"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text: str, max_chars: int, overlap: int) -> List[str]:\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return []\n",
        "    chunks = []\n",
        "    current = []\n",
        "    length = 0\n",
        "    for w in words:\n",
        "        if length + len(w) + 1 > max_chars:\n",
        "            chunk = \" \".join(current)\n",
        "            chunks.append(chunk)\n",
        "            if overlap > 0 and len(chunk) > overlap:\n",
        "                carry = chunk[-overlap:]\n",
        "                current = carry.split()\n",
        "                length = sum(len(x)+1 for x in current)\n",
        "            else:\n",
        "                current = []\n",
        "                length = 0\n",
        "        current.append(w)\n",
        "        length += len(w) + 1\n",
        "    if current:\n",
        "        chunks.append(\" \".join(current))\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvgEOGjQHYtt"
      },
      "outputs": [],
      "source": [
        "\n",
        "JSON_SCHEMA_EXACT = \"\"\"\n",
        "نمونه ساختار دقیق JSON برای هر استان:\n",
        "\n",
        "{\n",
        "  \\\"title\\\": \\\"string\\\",\n",
        "  \\\"location\\\": {\\\"province\\\":\\\"string\\\",\\\"city\\\":\\\"string\\\"},\n",
        "  \\\"geographical_features\\\":[{\\\"name\\\":\\\"string\\\",\\\"items\\\":[{\\\"name\\\":\\\"string\\\",\\\"images\\\":[\\\"string\\\"]}]}],\n",
        "  \\\"natural_resources\\\":[{\\\"name\\\":\\\"string\\\",\\\"description\\\":[\\\"string\\\"]}],\n",
        "  \\\"vegetation\\\":[\\\"string\\\"],\n",
        "  \\\"topography\\\":[{\\\"name\\\":\\\"string\\\",\\\"description\\\":[\\\"string\\\"]}],\n",
        "  \\\"tourist_attractions\\\":[{\\\"name\\\":\\\"string\\\",\\\"images\\\":[\\\"string\\\"],\\\"year_built\\\":\\\"string\\\",\\\"constructor\\\":\\\"string\\\",\\\"architect\\\":\\\"string\\\",\\\"description\\\":\\\"string\\\"}],\n",
        "  \\\"climate_impacts\\\":[{\\\"impact\\\":\\\"string\\\",\\\"description\\\":[\\\"string\\\"]}],\n",
        "  \\\"additional_info\\\":{\\\"books_source\\\":\\\"string\\\",\\\"other_sources\\\":[\\\"string\\\"]}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "EXAMPLE_JSON = \"\"\"\n",
        "{\n",
        "  \\\"title\\\": \\\"ویژگی‌های جغرافیایی اصفهان\\\",\n",
        "  \\\"location\\\": {\\\"province\\\":\\\"اصفهان\\\",\\\"city\\\":\\\"اصفهان\\\"},\n",
        "  \\\"geographical_features\\\":[\n",
        "    {\\\"name\\\": \\\"رودخانه‌ها\\\", \\\"items\\\":[{\\\"name\\\":\\\"زاینده‌رود\\\",\\\"images\\\":[]}]} ,\n",
        "    {\\\"name\\\": \\\"کوه‌ها\\\", \\\"items\\\":[{\\\"name\\\":\\\"کوه صفه\\\",\\\"images\\\":[]}]}\n",
        "  ],\n",
        "  \\\"natural_resources\\\":[],\n",
        "  \\\"vegetation\\\":[],\n",
        "  \\\"topography\\\":[],\n",
        "  \\\"tourist_attractions\\\":[],\n",
        "  \\\"climate_impacts\\\":[],\n",
        "  \\\"additional_info\\\":{\\\"books_source\\\":\\\"\\\",\\\"other_sources\\\":[]}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_TEMPLATE = f\"\"\"\n",
        "شما مدل GPT هستید و **تنها** باید یک شیء JSON یکتا و **معتبر** تولید کنید.\n",
        "۱. کلیدها و مقدارهای رشته‌ای حتماً با گیومهٔ دوگانه (\\\"\") باشند.\n",
        "۲. اگر داده‌ای وجود ندارد، از \\\"\\\" یا [] استفاده کنید؛ **هرگز** {{}} خالی ننویسید.\n",
        "۳. اگر برای فیلدی اطمینان کمتر از ۹۰٪ دارید یا داده نیست، آن را \\\"\\\" یا [] بگذارید.\n",
        "۴. حتماً حداقل یک مورد واقعی برای هر لیست استخراج‌شده در متن بیاورید.\n",
        "۵. **هرگز** JSON را داخل code fence (```…```) یا تگ Markdown قرار ندهید—فقط جسم خالص JSON را برگردانید!\n",
        "۶. فقط JSON خالص، بدون توضیح یا کامنت.\n",
        "۷. **هرگز** تنها مجموعهٔ نمونه (EXAMPLE_JSON) را به‌عنوان خروجی نهایی برنگردانید؛ حتماً داده‌های استخراج‌شده از متن ورودی را نمایش دهید.\n",
        "۸. در صورت نیاز برای تکمیل داده‌ها، ویکی‌پدیا و منابع آنلاین معتبر را جست‌وجو کرده و اطلاعات به‌روز را استخراج کنید.\n",
        "\n",
        "{JSON_SCHEMA_EXACT}\n",
        "\n",
        "{EXAMPLE_JSON}\n",
        "\n",
        "--- متن منبع ---\\n\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V27jMrRHc9M"
      },
      "outputs": [],
      "source": [
        "seen_entries = {\n",
        "    \"geographical_features\": set(),\n",
        "    \"natural_resources\": set(),\n",
        "    \"vegetation\": set(),\n",
        "    \"topography\": set(),\n",
        "    \"tourist_attractions\": set(),\n",
        "    \"climate_impacts\": set(),\n",
        "}\n",
        "seen_subitems = set()\n",
        "\n",
        "def process_chunk(chunk: str, idx: int) -> Optional[Dict[str, Any]]:\n",
        "    prompt = PROMPT_TEMPLATE + chunk + \"\\n\"\n",
        "    if idx <= 5:\n",
        "        console.rule(f\"[bold green]Chunk {idx} Prompt[/]\")\n",
        "        console.print(prompt)\n",
        "\n",
        "    try:\n",
        "        resp = openai.ChatCompletion.create(\n",
        "            model=CONFIG['model'],\n",
        "            messages=[{'role': 'user', 'content': prompt}],\n",
        "            max_tokens=CONFIG['max_tokens'],\n",
        "            temperature=CONFIG['temperature'],\n",
        "            n=1,\n",
        "        )\n",
        "        content = resp.choices[0].message.content.strip()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "    if idx <= 5:\n",
        "        console.rule(f\"[bold blue]Chunk {idx} GPT Output[/]\")\n",
        "        console.print(content)\n",
        "\n",
        "    raw = extract_json_block(content)\n",
        "    if not raw:\n",
        "        return None\n",
        "\n",
        "    raw = clean_json_block(raw)\n",
        "    raw = clean_json_string(raw)\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(raw)\n",
        "    except json.JSONDecodeError:\n",
        "        return None\n",
        "\n",
        "    if parsed:\n",
        "        for section in [\"geographical_features\", \"natural_resources\", \"topography\",\n",
        "                        \"tourist_attractions\", \"climate_impacts\"]:\n",
        "            new_list = []\n",
        "            for item in parsed.get(section, []):\n",
        "                name = item.get(\"name\")\n",
        "                if not name or name in seen_entries[section]:\n",
        "                    continue\n",
        "                seen_entries[section].add(name)\n",
        "\n",
        "                if section == \"geographical_features\":\n",
        "                    kept_subs = []\n",
        "                    for sub in item.get(\"items\", []):\n",
        "                        subname = sub.get(\"name\")\n",
        "                        if subname and subname not in seen_subitems:\n",
        "                            seen_subitems.add(subname)\n",
        "                            kept_subs.append(sub)\n",
        "                    item[\"items\"] = kept_subs\n",
        "                    if kept_subs:\n",
        "                        new_list.append(item)\n",
        "                else:\n",
        "                    new_list.append(item)\n",
        "            parsed[section] = new_list\n",
        "\n",
        "        for section in [\"vegetation\"]:\n",
        "            new_list = []\n",
        "            for val in parsed.get(section, []):\n",
        "                if not val:\n",
        "                    continue\n",
        "                val_key = json.dumps(val, sort_keys=True)\n",
        "                if val_key not in seen_entries[section]:\n",
        "                    seen_entries[section].add(val_key)\n",
        "                    new_list.append(val)\n",
        "            parsed[section] = new_list\n",
        "\n",
        "    if idx <= 5:\n",
        "        console.rule(f\"[bold magenta]Chunk {idx} Parsed & Deduped JSON[/]\")\n",
        "        console.print(json.dumps(parsed, ensure_ascii=False, indent=2))\n",
        "        console.rule()\n",
        "\n",
        "    return parsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNNs9PyYHf_Z"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    start = time.time()\n",
        "    console.rule(\"[bold green]Starting Extraction[/]\")\n",
        "    text = normalize_text(extract_text_from_pdf(CONFIG['pdf_path'], CONFIG['start_page'], CONFIG['end_page']))\n",
        "    chunks = chunk_text(text, CONFIG['chunk_size'], CONFIG['overlap_size'])\n",
        "    console.print(f\"[bold]Province:[/] {CONFIG['province']}\")\n",
        "    console.print(f\"[bold]Total chunks:[/] {len(chunks)}\\n\")\n",
        "    combined = {\"province\": CONFIG['province']}\n",
        "    partials = []\n",
        "    with Progress(SpinnerColumn(style=\"bold green\"), TextColumn(\"[progress.description]{task.description}\"), BarColumn(bar_width=None), TextColumn(\"[bold magenta]{task.completed}/{task.total} chunks\"), TimeElapsedColumn(), TimeRemainingColumn(), console=console) as progress:\n",
        "        task = progress.add_task(\"Extracting chunks\", total=len(chunks))\n",
        "        with ThreadPoolExecutor(max_workers=CONFIG['workers']) as executor:\n",
        "            futures = {executor.submit(process_chunk, chunk, idx+1): idx+1 for idx, chunk in enumerate(chunks)}\n",
        "            for fut in as_completed(futures):\n",
        "                res = fut.result()\n",
        "                if res:\n",
        "                    partials.append(res)\n",
        "                progress.advance(task)\n",
        "    for part in partials:\n",
        "        for k, v in part.items():\n",
        "            if k == \"province\":\n",
        "                continue\n",
        "            if k not in combined:\n",
        "                combined[k] = v\n",
        "            elif isinstance(v, list) and isinstance(combined[k], list):\n",
        "                combined[k].extend(v)\n",
        "    out_file = Path(f\"./{CONFIG['province']}_dataset.json\")\n",
        "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(combined, f, ensure_ascii=False, indent=2)\n",
        "    elapsed = time.time() - start\n",
        "    total_items = sum(len(v) if isinstance(v, list) else 1 for k, v in combined.items() if k != \"province\")\n",
        "    console.rule(\"[bold green]Extraction Complete[/]\")\n",
        "    console.print(f\"• Saved to: [bold]{out_file}[/]\")\n",
        "    console.print(f\"• Items: [bold]{total_items}[/]\")\n",
        "    console.print(f\"• Chunks: [bold]{len(chunks)}[/]\")\n",
        "    console.print(f\"• Time: [bold]{elapsed:.2f}s[/]\\n\")\n",
        "    table = Table(title=\"✅ Extraction Summary\")\n",
        "    table.add_column(\"Province\", style=\"cyan\")\n",
        "    table.add_column(\"Items\", justify=\"right\", style=\"magenta\")\n",
        "    table.add_column(\"Chunks\", justify=\"right\", style=\"magenta\")\n",
        "    table.add_column(\"Time (s)\", justify=\"right\", style=\"magenta\")\n",
        "    table.add_row(CONFIG['province'], str(total_items), str(len(chunks)), f\"{elapsed:.2f}\")\n",
        "    console.print(table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq1lYanTHj6P"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
